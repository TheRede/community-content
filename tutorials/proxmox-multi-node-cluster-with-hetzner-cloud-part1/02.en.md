---
SPDX-License-Identifier: MIT
path: "/tutorials/proxmox-multi-node-cluster-with-hetzner-cloud-part2"
slug: "proxmox-multi-node-cluster-with-hetzner-cloud-part2"
date: "2030-01-01"
title: "Proxmox multi-node cluster with Hetzner Cloud, Part 2."
short_description: "As part of this tutorial series you will get deeper in initialization of multi-node Proxmox cluster with help of Hetzner Cloud infrastructure. Whether you want to use Plex and attach 1TB cloud drive for movies and shows or setup the cluster for your business (routing, test and production environments, VMs) - you are in the right topic."
tags: ["Proxmox", "Caddy", "Cloudflare", "ProxmoxFirewall", "ZeroTier", "AtlassianJira", "AtlassianConfluence", "Pritunl", "VPN"]
author: "TheRede"
author_link: "https://github.com/TheRede"
author_img: ""
author_description: "Get a feel of enterprise environment with Proxmox Cluster delpoyed in the cloud in a couple hours!"
language: "en"
available_languages: ["en"]
header_img: ""
---
## Part 2. Initialize Proxmox node network for local containers to have internet access 
### Introduction
As part of this tutorial series you will get deeper in initialization of multi-node Proxmox cluster with help of Hetzner Cloud infrastructure. Whether you want to use Plex and attach 1TB cloud drive for movies and shows or setup the cluster for your business (routing, test and production environments, VMs) - you are in the right topic.

Our setup will be connected through Zerotier and include:
- Router instance - with Caddy and Cloudlare integration to secure the traffic, VPN for now;
- Dev server instance - Linux VMs and local services like API servers, Web-servers;
- Production server instance  - production services like Atlassian stack (Jira and Confluence) and other CentOS, Debian, Ubuntu LXC's.

Installation instructions on one node is available at Hetzner community tutorials :
https://community.hetzner.com/tutorials/install-and-configure-proxmox_ve?title=Proxmox_VE/en

Previous tutorials:
1. Quick installation of Proxmox on the nodes with help of Rescue system

Next tutorials:
3. Creating cluster network between Proxmox servers with Zerotier
4. Inialization of Firewall to secure Proxmox node
5. LXC and VM launch with Proxmox dev node
6. Installation and management of Caddy and Cloudflare as Proxmox cluster main web-proxy
7. Installing Pritunl as VPN endpoint in Proxmox router node
8. Installing Jira, Confluence and their databases on the Proxmox production node

### Prerequisites / What we use
1. Linux desktop @ Ubuntu 19.04
2. Internal terminal (you can pop it open pressing Ctrl+Alt+T) or Termius (Windows, Linux, MacOS, iOS, Android apps)
3. Local SSH keypair

### Network initialization on the node with NAT for containers and VMs
#### Reconfigure the node for proper work with bridges and reverse proxy
**This will give you option to have VM's and LXC's assigned additional IP's ordered from Hetzner without NAT, a.e. for Windows RDP machines or webserver LXC.**
1. ssh to the node with terminal `ssh root@your-server-ip`
2. check routes your server have now with `ip route`
 - write down the row with `default via your-gateway-ip dev ens3`, it stores info on the gateway of your instance
3. copy ip address information on network device ens3 with `ip a` after inet or simply copy the ip address from cloud console of Hetzner (the url will be like https://console.hetzner.cloud/projects/xxxxxx/servers/xxxxxx/overview)
4. in terminal type `nano /etc/network/interfaces`
5. in the file change row `iface ens3 inet dhcp` to `iface ens3 inet static`
6. add to the end of the file (address = your server ip address from `ip a`, gateway = your-gateway-ip from `ip route`)
```
auto vmbr0
iface vmbr0 inet static
        address xxx.xxx.xxx.xxx
        netmask 255.255.255.0
        gateway xxx.xxx.xxx
        bridge_ports ens3
        bridge_stp off
        bridge_fd 0
```
7. Press Ctrl+O, Enter, Ctrl+X to save changes and exit from the file
8. Reboot the server with `reboot` for changes to be applied
9. Check the new vmbr via Proxmox GUI -> Node -> Network
#### Enable masquerading for VMBR1 
**This will give local containers and VM's access to internet via additional VMBR hosting <10.10.0.0/24> network for example, it will not be available outside your network without VPN-ing to the network and adding routes. We will make it with Zerotier later.**
1. from terminal run `sysctl -w net.ipv4.ip_forward=1` to enable ipv4 forwarding to local machines.
2. `sysctl -w net.ipv6.conf.all.forwarding=1` same for ipv6
3. run `nano /etc/network/interfaces/`
4. add to the end of the file
```
auto vmbr1
iface vmbr1 inet static
        address  10.10.0.1
        netmask  255.255.255.0
        bridge_ports none
        bridge_stp off
        bridge_fd 0

        post-up echo 1 > /proc/sys/net/ipv4/ip_forward
        post-up   iptables -t nat -A POSTROUTING -s '10.10.0.0/24' -o vmbr0 -j MASQUERADE
        post-down iptables -t nat -D POSTROUTING -s '10.10.0.0/24' -o vmbr0 -j MASQUERADE
```
5. Reboot the server with `reboot` for changes to be applied
6. Check the new vmbr via Proxmox GUI -> Node -> Network
**DO NOT CREATE LXC's and VM's yet because we going to deploy the cluster in next steps and if you create containers or VM's it can crash the cluster work in different ways**
7. Repeat on other nodes

## Conclusion
We created the:
- external network to be used with VM's RDP and LXC webservers;
- local network for containers and in the next steps will configure communication between nodes using different networks (<10.10.0.0/24>, <10.10.1.0/24>, etc)

**License: MIT**
